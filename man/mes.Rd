% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mes.R
\name{mes}
\alias{mes}
\title{Mixed Exponential Smoothing}
\usage{
mes(y, model = "ZXZ", lags = c(frequency(y)), orders = list(ar = c(0), i =
  c(0), ma = c(0)), formula = NULL, distribution = c("default", "dnorm",
  "dlogis", "dlaplace", "dt", "ds", "dalaplace", "dlnorm", "dllaplace", "dls",
  "dinvgauss"), loss = c("likelihood", "MSE", "MAE", "HAM", "LASSO", "RIDGE",
  "MSEh", "TMSE", "GTMSE", "MSCE"), h = 0, holdout = FALSE,
  persistence = NULL, phi = NULL, initial = c("optimal", "backcasting"),
  occurrence = c("none", "auto", "fixed", "general", "odds-ratio",
  "inverse-odds-ratio", "direct"), ic = c("AICc", "AIC", "BIC", "BICc"),
  bounds = c("usual", "admissible", "none"), xreg = NULL,
  xregDo = c("use", "select"), xregInitial = NULL, xregPersistence = 0,
  silent = TRUE, ...)
}
\arguments{
\item{y}{Vector, containing data needed to be forecasted. If a matrix is
provided, then the first column is used as a response variable, while the rest
of the matrix is used as a set of explanatory variables.}

\item{model}{The type of ETS model. The first letter stands for the type of
the error term ("A" or "M"), the second (and sometimes the third as well) is for
the trend ("N", "A", "Ad", "M" or "Md"), and the last one is for the type of
seasonality ("N", "A" or "M"). In case of several lags, the seasonal components
are assumed to be the same. The model is then printed out as
MES(M,Ad,M)[m1,m2,...], where m1, m2, ... are the lags specified by the
\code{lags} parameter.
There are several options for the \code{model} besides the conventional ones,
which rely on information criteria:
\enumerate{
\item \code{model="ZZZ"} means that the model will be selected based on the
chosen information criteria type. The Branch and Bound is used in the process.
\item \code{model="XXX"} means that only additive components are tested, using
Branch and Bound.
\item \code{model="YYY"} implies selecting between multiplicative components.
\item \code{model="CCC"} trigers the combination of forecasts of models using
information criteria weights (Kolassa, 2011).
\item combinations between these four and the classical components are also
accepted. For example, \code{model="CAY"} will combine models with additive
trend and either none or multiplicative seasonality.
\item \code{model="PPP"} will produce the selection between pure additive and
pure multiplicative models. "P" stands for "Pure". This cannot be mixed with
other types of components.
\item \code{model="FFF"} will select between all the 30 types of models. "F"
stands for "Full". This cannot be mixed with other types of components.
\item The parameter \code{model} can also be a vector of names of models for a
finer tuning (pool of models). For example, \code{model=c("ANN","AAA")} will
estimate only two models and select the best of them.
}

Also, \code{model} can accept a previously estimated mes model and use all
its parameters.

Keep in mind that model selection with "Z" components uses Branch and Bound
algorithm and may skip some models that could have slightly smaller
information criteria. If you want to do a exhaustive search, you would need
to list all the models to check as a vector.

The default value is set to \code{"ZXZ}, because the multiplicative trend is explosive
and dangerous. It should be used only for each separate time series, not for the
atomated predictions for big datasets.}

\item{lags}{Defines lags for the corresponding components. All components
count, starting from level, so ETS(M,M,M) model for monthly data will have
lags=c(1,1,12). If fractional numbers are provided, then it is assumed that
the data is not periodic. The parameter \code{date} is then needed in order
to setup the appropriate time series structure.}

\item{orders}{The order of ARIMA to be included in the model. This should be passed
either as a vector (in which case the non-seasonal ARIMA is assumed) or as a list of
a type \code{orders=list(ar=c(p,P),i=c(d,D),ma=c(q,Q))}, in which case the \code{lags}
variable is used in order to determine the seasonality m. See \link[smooth]{msarima}
for details.}

\item{formula}{Formula to use in case of explanatory variables. If \code{NULL},
then all the variables are used as is. Only considered if \code{xreg} is not
\code{NULL} and \code{xregDo="use"}.}

\item{distribution}{what density function to assume for the error term. The full
name of the distribution should be provided, starting with the letter "d" -
"density". The names align with the names of distribution functions in R.
For example, see \link[stats]{dnorm}. For detailed explanation of available
distributions, see vignette in greybox package: \code{vignette("greybox","alm")}.}

\item{loss}{The type of Loss Function used in optimization. \code{loss} can
be:
\itemize{
\item \code{likelihood} - the model is estimated via the maximisation of the
likelihood of the function specified in \code{distribution};
\item \code{MSE} (Mean Squared Error),
\item \code{MAE} (Mean Absolute Error),
\item \code{HAM} (Half Absolute Moment),
\item \code{LASSO} - use LASSO to shrink the parameters of the model;
\item \code{RIDGE} - use RIDGE to shrink the parameters of the model;
\item \code{TMSE} - Trace Mean Squared Error,
\item \code{GTMSE} - Geometric Trace Mean Squared Error,
\item \code{MSEh} - optimisation using only h-steps ahead error,
\item \code{MSCE} - Mean Squared Cumulative Error.
}
Note that model selection and combination works properly only for the default
\code{loss="likelihood"}.

There are also available analytical approximations for multistep functions:
\code{aMSEh}, \code{aTMSE} and \code{aGTMSE}. These can be useful in cases
of small samples.

Finally, just for fun the absolute and half analogues of multistep estimators
are available: \code{MAEh}, \code{TMAE}, \code{GTMAE}, \code{MACE},
\code{HAMh}, \code{THAM}, \code{GTHAM}, \code{CHAM}.}

\item{h}{The forecast horizon. Mainly needed for the multistep loss functions.}

\item{holdout}{Logical. If \code{TRUE}, then the holdout of the size \code{h}
is taken from the data (can be used for the model testing purposes.}

\item{persistence}{Persistence vector \eqn{g}, containing smoothing
parameters. If \code{NULL}, then estimated.}

\item{phi}{Value of damping parameter. If \code{NULL} then it is estimated.
Only applicable for damped-trend models.}

\item{initial}{Can be either character or a vector of initial states. If it
is character, then it can be \code{"optimal"}, meaning that the initial
states are optimised, or \code{"backcasting"}, meaning that the initials are
produced using backcasting procedure (advised for data with high frequency).}

\item{occurrence}{The type of model used in probability estimation. Can be
\code{"none"} - none,
\code{"fixed"} - constant probability,
\code{"general"} - the general Beta model with two parameters,
\code{"odds-ratio"} - the Odds-ratio model with b=1 in Beta distribution,
\code{"inverse-odds-ratio"} - the model with a=1 in Beta distribution,
\code{"direct"} - the TSB-like (Teunter et al., 2011) probability update
mechanism a+b=1,
\code{"auto"} - the automatically selected type of occurrence model.

The type of model used in the occurrence is equal to the one provided in the
\code{model} parameter.

Also, a model produced using \link[smooth]{oes} or \link[greybox]{alm} function
can be used here.}

\item{ic}{The information criterion to use in the model selection / combination
procedure.}

\item{bounds}{The type of bounds for the persistence to use in the model
estimation. Can be either \code{admissible} - guaranteeing the stability of the
model, \code{traditional} - restricting the values with (0, 1) or \code{none} - no
restrictions (potentially dangerous).}

\item{xreg}{The vector (either numeric or time series) or the matrix (or
data.frame / data.table) of exogenous variables that should be included in the
model. If matrix is included than columns should contain variables and rows -
observations.
Note that \code{xreg} should have number of observations equal to
the length of the response variable \code{y}. If it is not equal, then the
function will either trim or extrapolate the data.}

\item{xregDo}{The variable defines what to do with the provided xreg:
\code{"use"} means that all of the data should be used, while
\code{"select"} means that a selection using \code{ic} should be done.}

\item{xregInitial}{The vector of initial parameters for exogenous variables.
Ignored if \code{xreg} is NULL.}

\item{xregPersistence}{The persistence vector \eqn{g_X}, containing smoothing
parameters for exogenous variables. If \code{NULL}, then estimated. If \code{0}
then each element of the vector is set to zero. Prerequisite - non-NULL \code{xreg}.}

\item{silent}{Specifies, whether to provide the progress of the function or not.
If \code{TRUE}, then the function will print what it does and how much it has
already done.}

\item{...}{Other non-documented parameters. For example \code{FI=TRUE} will
make the function also produce Fisher Information matrix, which then can be
used to calculated variances of smoothing parameters and initial states of
the model. This is used in the \link[stats]{vcov} method.
Starting values of parameters can be passed via \code{B}, while the upper and lower
bounds should be passed in \code{ub} and \code{lb} respectively. In this case they
will be used for optimisation. These values should have the length equal
to the number of parameters to estimate in the following order:
\enumerate{
\item All smoothing parameters (for the states and then for the explanatory variables);
\item Damping parameter (if needed);
\item All the initial values (for the states and then for the explanatory variables).
}
You can also pass parameters to the optimiser in order to fine tune its work:
\itemize{
\item \code{maxeval} - maximum number of evaluations to carry out (default is 200 for
data with lags <= 12 and 1000 for the larger lags);
\item \code{maxtime} - stop, when the optimisation time (in seconds) exceeds this;
\item \code{xtol_rel} - the relative precision of the optimiser (the default is 1E-6);
\item \code{xtol_abs} - the absolute precision of the optimiser (the default is 0 -
not used);
\item \code{algorithm} - the algorithm to use in optimisation
(\code{"NLOPT_LN_BOBYQA"} by default);
\item \code{print_level} - the level of output for the optimiser (0 by default).
}
You can read more about these parameters by running the function
\link[nloptr]{nloptr.print.options}.
Finally, the parameter \code{lambda} for LASSO / RIDGE, Asymmetric Laplace and df
of Student's distribution can be provided here as well.}
}
\value{
Object of class "mes" is returned. It contains the list of the
following values:
\itemize{
\item \code{model} - the name of the constructed model,
\item \code{timeElapsed} - the time elapsed for the estimation of the model,
\item \code{y} - the in-sample part of the data used for the training of the model,
\item \code{holdout} - the holdout part of the data, excluded for purposes of model evaluation,
\item \code{fitted} - the vector of fitted values,
\item \code{residuals} - the vector of residuals,
\item \code{forecast} - the point forecast for h steps ahead (by default NA is returned),
\item \code{states} - the matrix of states with observations in rows and states in columns,
\item \code{persisten} - the vector of smoothing parameters,
\item \code{phi} - the value of damping parameter,
\item \code{transition} - the transition matrix,
\item \code{measurement} - the measurement matrix with observations in rows and state elements
in columns,
\item \code{initialType} - the type of initialisation used ("optimal" / "backcasting" / "provided"),
\item \code{initial} - the initial values, including level, trend and seasonal components,
\item \code{nParam} - the matrix of the estimated / provided parameters,
\item \code{occurrence} - the oes model used for the occurrence part of the model,
\item \code{xreg} - the matrix of explanatory variables after all expansions and transformations,
\item \code{xregInitial} - the vector of initials for the parameters of explanatory variable,
\item \code{xregPersistence} - the vector of smoothing parameters for the explanatory variables,
\item \code{formula} - the formula used for the explanatory variables expansion,
\item \code{loss} - the type of loss function used in the estimation,
\item \code{lossValue} - the value of that loss function,
\item \code{logLik} - the value of the log-likelihood,
\item \code{distribution} - the distribution function used in the calculation of the likelihood,
\item \code{scale} - the value of the scale parameter,
\item \code{lambda} - the value of the parameter used in LASSO / dalaplace / dt,
\item \code{B} - the vector of all estimated parameters,
\item \code{lags} - the vector of lags used in the model construction.
}
}
\description{
Function constructs an advanced Single Source of Error model, based on ETS
taxonomy.
}
\details{
Function estimates ETS in a form of the Single Source of Error state space
model of the following type:

\deqn{y_{t} = o_t (w(v_{t-l}) + h(x_t, a_{t-1}) + r(v_{t-l}) \epsilon_{t})}

\deqn{v_{t} = f(v_{t-l}, a_{t-1}) + g(v_{t-l}, a_{t-1}, x_{t}) \epsilon_{t}}

Where \eqn{o_{t}} is the Bernoulli distributed random variable (in case of
normal data it equals to 1 for all observations), \eqn{v_{t}} is the state
vector and \eqn{l} is the vector of lags, \eqn{x_t} is the vector of
exogenous variables. w(.) is the measurement function, r(.) is the error
function, f(.) is the transition function, g(.) is the persistence
function and \eqn{a_t} is the vector of parameters for exogenous variables.
Finally, \eqn{\epsilon_{t}} is the error term.

The implemented model allows introducing several seasonal states and supports
intermittent data via the \code{occurrence} variable.

The error term \eqn{\epsilon_t} can follow different distributions, which
are regulated via the \code{distribution} parameter. This includes:
\enumerate{
\item \code{default} - Normal distribution is used for the Additive error models,
Inverse Gaussian is used for the Multiplicative error models.
\item \link[stats]{dnorm} - Normal distribution,
\item \link[stats]{dlogis} - Logistic Distribution,
\item \link[greybox]{dlaplace} - Laplace distribution,
\item \link[greybox]{dalaplace} - Asymmetric Laplace distribution,
\item \link[stats]{dt} - T-distribution,
\item \link[greybox]{ds} - S-distribution,
\item \link[stats]{dlnorm} - Log normal distribution,
\item dllaplace - Log Laplace distribution,
\item ds - Log S distribution,
\item \link[statmod]{dinvgauss} - Inverse Gaussian distribution,
}

For some more information about the model and its implementation, see the
vignette: \code{vignette("mes","mes")}.
}
\examples{

# Model selection using a specified pool of models
ourModel <- mes(rnorm(100,100,10), model=c("ANN","ANA","AAA"), lags=c(5,10))

summary(ourModel)
forecast(ourModel)
plot(forecast(ourModel))

# Model combination using a specified pool
ourModel <- mes(rnorm(100,100,10), model=c("ANN","AAN","MNN","CCC"), lags=c(5,10))

}
\references{
\itemize{
\item Snyder, R. D., 1985. Recursive Estimation of Dynamic Linear Models.
Journal of the Royal Statistical Society, Series B (Methodological) 47 (2), 272-276.
\item Hyndman, R.J., Koehler, A.B., Ord, J.K., and Snyder, R.D. (2008)
Forecasting with exponential smoothing: the state space approach,
Springer-Verlag. \url{http://dx.doi.org/10.1007/978-3-540-71918-2}.
}

\itemize{
\item Svetunkov Ivan and Boylan John E. (2017). Multiplicative
State-Space Models for Intermittent Time Series. Working Paper of
Department of Management Science, Lancaster University, 2017:4 , 1-43.
\item Teunter R., Syntetos A., Babai Z. (2011). Intermittent demand:
Linking forecasting to inventory obsolescence. European Journal of
Operational Research, 214, 606-615.
\item Croston, J. (1972) Forecasting and stock control for intermittent
demands. Operational Research Quarterly, 23(3), 289-303.
\item Syntetos, A., Boylan J. (2005) The accuracy of intermittent demand
estimates. International Journal of Forecasting, 21(2), 303-314.
}

\itemize{
\item Kolassa, S. (2011) Combining exponential smoothing forecasts using Akaike
weights. International Journal of Forecasting, 27, pp 238 - 251.
}

\itemize{
\item Taylor, J.W. and Bunn, D.W. (1999) A Quantile Regression Approach to
Generating Prediction Intervals. Management Science, Vol 45, No 2, pp
225-237.
\item Lichtendahl Kenneth C., Jr., Grushka-Cockayne Yael, Winkler
Robert L., (2013) Is It Better to Average Probabilities or
Quantiles? Management Science 59(7):1594-1611. DOI:
[10.1287/mnsc.1120.1667](https://doi.org/10.1287/mnsc.1120.1667)
}
}
\seealso{
\code{\link[forecast]{ets}, \link[smooth]{es}}
}
\author{
Ivan Svetunkov, \email{ivan@svetunkov.ru}
}
\keyword{models}
\keyword{nonlinear}
\keyword{regression}
\keyword{smooth}
\keyword{ts}
\keyword{univar}
